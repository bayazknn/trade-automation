# Recommended HYPERPARAM_CONFIGS based on log analysis (run_117271c5)
# Generated by LSTMLogAnalyzer
#
# Key changes from analysis:
# - focal_gamma: Narrowed to [1.0, 3.0] - optimal values are low
# - dropout: Expanded upper to 0.35 (was hitting upper bound)
# - hidden_size: Expanded lower to 64 (was hitting lower bound proximity)
# - num_layers: Expanded upper to 4 (was hitting upper bound)
# - label_smoothing: Expanded upper to 0.10 (was hitting upper bound)
# - batch_size: Expanded upper to 128 (was hitting upper bound)
# - all_hold_weight: Expanded lower to 1.8 (was hitting lower bound)

HYPERPARAM_CONFIGS = [
    HyperparamConfig('class_weight_power', 0.55, 0.62, 'float', 'class_weight_power'),
    HyperparamConfig('focal_gamma', 1.0, 3.0, 'float', 'focal_gamma'),
    HyperparamConfig('learning_rate', 0.00003, 0.0014, 'float', 'learning_rate'),
    HyperparamConfig('dropout', 0.17, 0.35, 'float', 'dropout'),
    HyperparamConfig('hidden_size', 64, 215, 'int', 'hidden_size'),
    HyperparamConfig('num_layers', 2, 4, 'int', 'num_layers'),
    HyperparamConfig('weight_decay', 0.008, 0.0112, 'float', 'weight_decay'),
    HyperparamConfig('label_smoothing', 0.048, 0.10, 'float', 'label_smoothing'),
    HyperparamConfig('batch_size', 75, 128, 'int', 'batch_size'),
    HyperparamConfig('all_hold_weight', 1.8, 3.5, 'float', 'all_hold_weight'),
    HyperparamConfig('entry_exit_weight', 1.17, 1.32, 'float', 'entry_exit_weight'),
    HyperparamConfig('scheduler_patience', 5, 11, 'int', 'scheduler_patience'),
    HyperparamConfig('input_seq_length', 12, 12, 'int', 'input_seq_length'),
]

# Recommended optimizer instantiation with seed for reproducibility:
"""
optimizer = LSTMMetaheuristicOptimizer(
    df=df,
    pop_size=15,
    iterations=75,
    n_workers=10,
    epochs_per_eval=100,
    checkpoint_interval=5,
    np_neighbors=2,
    pf_max=0.15,
    elitist_selection=True,
    elitist_constant=0.3,
    verbose=True,
    enable_logging=True,
    seed=42,  # Fixed seed for reproducibility
)
result = optimizer.optimize()

# Train from result with same seed (automatic)
trainer = optimizer.train_from_result(result, epochs=100)
"""
